
<section>
<h2>Recent News</h2>
<div class="news-item">
<div class="news-date">Feb 2026</div>
<div class="news-content">Three papers accepted to <strong>ICLR 2026</strong>: MuonBP, Scaling Laws for Inference-Efficient LLMs, and Speculative Decoding</div>
</div>
<div class="news-item">
<div class="news-date">Feb 2026</div>
<div class="news-content">Presented tutorial at <strong>AAAI 2026</strong>: "Algorithms and Systems for Efficient Inference in Generative AI"</div>
</div>
<div class="news-item">
<div class="news-date">Jan 2026</div>
<div class="news-content">Paper accepted to <strong>AISTATS 2026</strong>: Demystifying Transition Matching</div>
</div>
</section>
<section>
<h2>Biography</h2>
<div class="bio">
<p>
                        I am a <strong>Senior Applied Scientist and Research Lead</strong> at Amazon Web Services AI, where I lead the Core Algorithm team at AWS Annapurna Science. I manage a research organization of 14+ applied scientists, Amazon scholars, and research interns, developing algorithms that power foundation models across Amazon Bedrock, AGI, and Anthropic partnerships. I have co-authored 35+ papers at top ML conferences including ICLR, ICML, NeurIPS, AISTATS, and KDD.
                    </p>
<p>
<strong>Research Interests:</strong>
<ul style="margin-top: 6px; margin-bottom: 0; padding-left: 30px;">
<li>Hardware-aware model architectures and fine-grained scaling laws for high-throughput systems</li>
<li>Efficient training via low-precision methods and system-aware optimizers</li>
<li>Post-training and alignment through RLVR-based approaches and robust reward modeling</li>
<li>Generative AI for systems, including DSL kernel synthesis via inference-time scaling</li>
</ul>
</p>
<p>
                        I received my Ph.D. in Electrical Engineering from Stanford University in 2020, where I was advised by Stephen P. Boyd and Jure Leskovec. I also earned my M.S. from Stanford in 2016 and my B.S. from KAIST in 2013 (Summa Cum Laude).
                    </p>
</div>
</section>
