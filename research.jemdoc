# jemdoc: menu{MENU}{research.html}
= Research


#~~~
#{}
#Based on the background on optimization, my research interests lie in developing methods of scalable algorithms for machine learning and reinforcement learning with theoretical guarantees. 

# My research interests lie in optimization, machine learning, and reinforcement learning. With these combinations, I develop methods related to modelings and algorithms in pursuit that AI problems can be solved with scalable and robust methods with theoretical guarantees.
#~~~

== Sorted by Topics


*Reinforcement Learning and Optimal Control*
- Structured Policy Iteration for Linear Quadratic Regulator. \n
*Y. Park* et al.\n
Accepted for /International Conference on Machine Learning/ (ICML).

- [http://www.stanford.edu/~youngsuk/papers/battery_optimization.pdf Optimal Operation of a Plug-in Hybrid Vehicle with Battery Thermal and Degradation Model.] \n
J. Kim, *Y. Park*, J. Fox, S. Boyd, and W. Dally. \n
To appear in /Proceedings of the American Control Conference (ACC)/, 2020.\n

- [http://www.stanford.edu/~youngsuk/papers/LQRcloud.pdf Linear Quadratic Regulator for Resource-Efficient Cloud Services.] \n
*Y. Park*, K. Mahadik, R. Rossi, G. Wu, and H. Zhao. \n
/Proceedings of ACM Symposium on Cloud Computing (SoCC)/, 2019. #\[[http://www.stanford.edu/~youngsuk/papers/LQRcloud.pdf Abstract]\]\n

- Convergent Actor-Critic under Off-policy and Function Approximation. \n
H. Maei and *Y. Park* \n
 \[[http://www.stanford.edu/~youngsuk/papers/gac_poster.pdf Slides]\].

*Convex Optimization*
- [https://arxiv.org/abs/1910.07056 Variable Metric Proximal Gradient Method with Diagonal Barzilai-Borwein Stepsize.] \n
*Y. Park*, S. Dhar, S. Boyd, and M. Shah. \n
/Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP)/, 2020. \n 
\[[https://arxiv.org/abs/1910.07056 Arxiv (long version)]\], \[[http://www.stanford.edu/~youngsuk/papers/vmpg_icassp.pdf ICASSP]\], \[[http://www.stanford.edu/~youngsuk/papers/nips_vmpg.pdf NeurIPS Workshop]\]

- [https://link.springer.com/article/10.1007/s11590-019-01520-y Linear Convergence of Cyclic SAGA.] \n
*Y. Park* and E. K. Ryu. \n
/Optimization Letter/, 2020. \n
\[[https://arxiv.org/abs/1810.11167  Arxiv]\]



*Machine Learning* 
- Structured Neural Inference for Undirected Graphical Model from Heterogeneous Data.  \n
*Y. Park* et al. \n
In preparation. # to submit /Neural Information Processing Systems (NeurIPS)/.\n

- [http://www.stanford.edu/~youngsuk/papers/TVGL.pdf Network Inference via the Time-Varying Graphical Lasso.]\n 
D. Hallac, *Y. Park*, S. Boyd, and J. Leskovec. \n
/Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)/, 2017 \n
\[[https://github.com/davidhallac/TVGL Code]\]

- [http://stanford.edu/~boyd/papers/pairwise_exp_struct.html Learning the Network Structure of Heterogeneous Data via Pairwise Exponential Markov Random Fields.] \n
*Y. Park*, D. Hallac, S. Boyd, and J. Leskovec. \n
/Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS)/, 2017 \n
\[[http://www.stanford.edu/~youngsuk/papers/AISTATS_PE-MRFs_Proof.pdf Supplementary Material]\], \[[https://github.com/youngsuk0723/PE-MRF-Code Code]\]
#http://proceedings.mlr.press/v54/park17d.html , PMLR 54:1302-1310, 2017.




*Other Topics*# (Information Theory)
- Universal Loseless Compression: Context Tree Weighting. \[[http://www.stanford.edu/~youngsuk/papers/slide_universal_compression.pdf Slides]\]
- Hypercontractivity, Maximal Correlation, and Non-cooperative Simulation. \[[http://www.stanford.edu/~youngsuk/papers/slide_measure_correlation.pdf Slides], [http://www.stanford.edu/~youngsuk/papers/report_measure_correlation.pdf Report]\]
- Successive Lossy Compression for Laplacian Source. \[[http://www.stanford.edu/~youngsuk/papers/slide_lossycompression.pdf Slides], [http://www.stanford.edu/~youngsuk/papers/report_lossycompression.pdf Report]\]

#= 
#See [https://scholar.google.com/citations?user=jWROvQ0AAAAJ&hl=en&oi=sra Google Scholar] sorted by citation count.
